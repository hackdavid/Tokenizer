{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqVgn41T/szfwTk98aTLDY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hackdavid/Tokenizer/blob/main/character_leve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6siHiDmzS73l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer with its type\n"
      ],
      "metadata": {
        "id": "SSTmXdEDTWs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Character Level Tokenizer\n",
        "\n",
        "*   first i will implement character level tokenizer alogrithm\n",
        "*   train our tokenizer using custome dataset using our own alogrithm\n",
        "*   train our tokenizer with sentencepiece libray\n",
        "*   train our tokenizer with huggingface\n",
        "*.  we see the difference tokenizer train by different different approches as mention above\n",
        "\n"
      ],
      "metadata": {
        "id": "msZiO0UxU-Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first download the data that are going to used in this whole process\n",
        "# get dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfGyIFc-ZMZw",
        "outputId": "6b7d4517-b844-4e37-b4e7-6612a4cbbf1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-24 17:34:06--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-11-24 17:34:06 (27.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/input.txt'"
      ],
      "metadata": {
        "id": "EKBJhYtWZYpr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def character_level_tokenizer_alogrithm(file_path:str,special_token=[],output_path:str='tokenizer.json'):\n",
        "  # special_token - read below to know more about special_token\n",
        "  # in character level simply we store only unique character with socre/index(integer) which will use for decoding\n",
        "\n",
        "  # read the file\n",
        "  text = open(file_path,'r')\n",
        "  text_with_lines = text.readlines()\n",
        "  # we are going assign space also with a index and \\n will also including\n",
        "  all_chars = {}\n",
        "  indx = 0\n",
        "  print(indx)\n",
        "  # adding special token at first\n",
        "  if special_token:\n",
        "    for token in special_token:\n",
        "      all_chars.update({token:indx})\n",
        "      indx += 1\n",
        "  for each_line in text_with_lines:\n",
        "    for i in each_line:\n",
        "      if i.lower() not in all_chars:\n",
        "        # make sure all character in lower case\n",
        "        all_chars.update({i.lower():indx})\n",
        "        indx += 1\n",
        "  # save this dict into .json file (you can store in any format ,i am using .json because hugginface store into .json but sentencepiece store in .vocab)\n",
        "  with open(output_path,'w') as f:\n",
        "    data = json.dumps(all_chars)\n",
        "    f.write(data)\n",
        "  print(f'Tokenizer is save with file_name {output_path}')\n",
        "  return all_chars\n",
        "\n",
        "# lets call the our function\n",
        "vocab = character_level_tokenizer_alogrithm(dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rozo7Kh_W-S9",
        "outputId": "bc49d730-36b7-405c-847b-9d2f918a7e8b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Tokenizer is save with file_name tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_s0Pt9B1e5Ij"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see the size of vocab and all the vocab\n",
        "print(f'vocab size: {len(vocab)}')\n",
        "print(vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILUg93wnZtiu",
        "outputId": "32587a03-9ab5-4dcf-9ceb-9115327be0f4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 39\n",
            "{'f': 0, 'i': 1, 'r': 2, 's': 3, 't': 4, ' ': 5, 'c': 6, 'z': 7, 'e': 8, 'n': 9, ':': 10, '\\n': 11, 'b': 12, 'o': 13, 'w': 14, 'p': 15, 'd': 16, 'a': 17, 'y': 18, 'u': 19, 'h': 20, ',': 21, 'm': 22, 'k': 23, '.': 24, 'l': 25, 'v': 26, '?': 27, \"'\": 28, 'g': 29, ';': 30, '!': 31, 'j': 32, '-': 33, 'q': 34, 'x': 35, '&': 36, '3': 37, '$': 38}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if you are famliar with tokenizer then there are some specail which all kept inside vocab so we have to also add that special token and all special token indx/score will start from 0 becuase we add these special token at starting\n",
        "\n",
        "lets see the special token(maybe i miss some special token but you can add by yourself)\n",
        "\n",
        "1. sos(start of sentence)\n",
        "2. eos(end of sentence)\n",
        "3. padding\n",
        "4. unknow token\n"
      ],
      "metadata": {
        "id": "ndc77YQSaJv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets define our special token\n",
        "'''\n",
        "sos = <s>\n",
        "eos = </s>\n",
        "padding = <padding>\n",
        "ukown  = <unk>\n",
        "'''\n",
        "special_token = ['<s>','</s>','<unk>','<padding>']\n",
        "vocab = vocab = character_level_tokenizer_alogrithm(dataset_path,special_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B1ngk6vaGE9",
        "outputId": "e3a16281-67fe-404b-fba9-e9b3e441d4b1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Tokenizer is save with file_name tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets the vocab\n",
        "# lets see the size of vocab and all the vocab\n",
        "print(f'vocab size: {len(vocab)}')\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BypZ-AE_eeZw",
        "outputId": "5ce44d5e-1ff8-4f03-f45a-53423edb7bc8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 43\n",
            "{'<s>': 0, '</s>': 1, '<unk>': 2, '<padding>': 3, 'f': 4, 'i': 5, 'r': 6, 's': 7, 't': 8, ' ': 9, 'c': 10, 'z': 11, 'e': 12, 'n': 13, ':': 14, '\\n': 15, 'b': 16, 'o': 17, 'w': 18, 'p': 19, 'd': 20, 'a': 21, 'y': 22, 'u': 23, 'h': 24, ',': 25, 'm': 26, 'k': 27, '.': 28, 'l': 29, 'v': 30, '?': 31, \"'\": 32, 'g': 33, ';': 34, '!': 35, 'j': 36, '-': 37, 'q': 38, 'x': 39, '&': 40, '3': 41, '$': 42}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this is simple alogrithm and used most of the NLP task becuase it takse less space and cover all the word by using these characters becase these are the base characters of any langauge ."
      ],
      "metadata": {
        "id": "zHBOXk2qfwrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets build a tokenizer traniner so that can we use further for traning our custome tokenzier"
      ],
      "metadata": {
        "id": "CRqUsylHhbQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. character level tokenizer\n",
        "# full code to train character_level tokenizer and i will use this for further in this series of NLP so make sure follow and save this code in your repo\n",
        "from typing import List\n",
        "import json\n",
        "class CharacterLevelTokenizerTranier:\n",
        "  def __int__(self,file_path:str,speical_token:List=[],tokenizer_path:str='tokenizer'):\n",
        "    self.file_path = file_path\n",
        "    self.special_token = speical_token\n",
        "    self.tokenizer_path = f'{tokenizer_path}.json'\n",
        "    self.vocab = {}\n",
        "  def train(self):\n",
        "    f = open(self.file_path,'r')\n",
        "    all_text = f.readlines()\n",
        "    index = 0\n",
        "    # adding special token first\n",
        "    if self.special_token:\n",
        "      for token in self.special_token:\n",
        "        if token not in self.vocab:\n",
        "          self.vocab.update({token:index})\n",
        "          index += 1\n",
        "    # adding all character in lower_case\n",
        "    for each_line in all_text:\n",
        "      for chr in each_line:\n",
        "        chr = chr.lower()\n",
        "        if chr not in self.vocab:\n",
        "          self.vocab.update({chr:index})\n",
        "          index += 1\n",
        "    # saving vocab in .json file\n",
        "    with open(self.tokenizer_path,'w') as f:\n",
        "      data = json.dumps(self.vocab)\n",
        "      f.write(data)\n",
        "    print(f'Tokenizer is trained and save to file {self.tokenizer_path}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qo1DDGpWTbod"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gUz1l9eriIcC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}