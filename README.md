# Tokenizer
Let's start Natural langauge processing from tokenizer 
As we all now tokenizer is base of NLP and there are different different kind of tokenizer
and all the people are usig it based on their purpose 
So lets Implemenation of all the tokenizer algorithm in python, i am not going to explain all theory in this
repo becuase you can find theory all over internet like huggingface or blogspost.
# Key Concepts
  1. write custome code for all tokenizer alogrithm
  2. Train custome tokenizer using Huggingface and sentencepiece
  3. Use pretrained tokenizer in huggingface and sentencepiece for inference

# type of tokenizers
There are mostly 4 type of tokenizer used in NLP
  1. character level tokenizer
  2. Byte pair Encoding(BPE) tokenizer
  3. word leve tokenzier
  4. sentence level tokenizer

In this repo i am going talk about first 3 tokenizer i.e character,BPE, word because sentence level tokenizer is not used mostly.

# 1. Character level tokenizer 
In this section i am going to implement a simple tokenizer which is character level tokenizer and i will write custome code for this implement 
and also train our tokenizer using custome code , huggingface and sentenicepiece . so lets start and also you can go through the notebook attached to this repo
