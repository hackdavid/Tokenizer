{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets see how bpe algorithm works and how encode and decode work "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take an simple string of text\n",
    "# reference : https://sidsite.com/posts/bpe/\n",
    "text = \"aa abc abc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-03 15:13:22--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.99MB/s    in 0.4s    \n",
      "\n",
      "2023-12-03 15:13:23 (2.99 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first download the data that are going to used in this whole process\n",
    "# get dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, ' ': 1, 'b': 1, 'c': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a dictnary to store our all token\n",
    "tokens = {}\n",
    "\n",
    "# add all the base character first and their count will be 1 as default\n",
    "tokens = {i:1 for i in text if i not in tokens}\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token : ('ab', 2)\n",
      "Pairs of Token : {'aa': 1, 'a ': 1, 'ab': 2, 'bc': 2, 'c ': 1}\n"
     ]
    }
   ],
   "source": [
    "# define a function that will return all pair of text having occurance in whole text\n",
    "def get_pairs(text):\n",
    "    pairs = {}\n",
    "    next_token = None\n",
    "    next_token_count = 0\n",
    "\n",
    "    for i in range(len(text)-1):\n",
    "        # skip (\" \", <tok>) to avoid counting across words\n",
    "        if text[i] == ' ':\n",
    "            continue\n",
    "        temp = text[i:i+2]\n",
    "        if temp in pairs:\n",
    "            pairs[temp] = pairs[temp] + 1\n",
    "        else:\n",
    "            pairs.update({temp:1})\n",
    "        if pairs[temp] > next_token_count:\n",
    "            next_token = temp\n",
    "            next_token_count = pairs[temp]\n",
    "    return pairs,(next_token,next_token_count)\n",
    "\n",
    "pairs,next_token = get_pairs(text)\n",
    "print(f'Next token : {next_token}')\n",
    "print(f'Pairs of Token : {pairs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, ' ': 1, 'b': 1, 'c': 1, 'ab': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after getting next token , add next_token to our tokens dictinary\n",
    "tokens.update({next_token[0]: next_token[1]})\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa a  ab b  bc c \n",
      "Next token : ('a ', 2)\n",
      "Pairs of Token : {'aa': 1, 'a ': 2, 'ab': 1, 'b ': 2, 'bc': 1, 'c ': 2}\n"
     ]
    }
   ],
   "source": [
    "# now repeate this\n",
    "new_text = ' '.join(list(pairs.keys()))\n",
    "print(new_text)\n",
    "pairs,next_token = get_pairs(new_text)\n",
    "print(f'Next token : {next_token}')\n",
    "print(f'Pairs of Token : {pairs}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# huggingface tokenizer demo and know how to use encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'hf_tokenizer.json'\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode output : Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "encoded ids [85, 81, 56, 99, 505, 75, 573, 63, 163, 50, 174, 244, 176, 57, 134]\n",
      "encoded token ['he', 'll', 'o', 'my', 'name', 'is', 'da', 'v', 'id', 'i', 'am', 'from', 'ne', 'p', 'al']\n"
     ]
    }
   ],
   "source": [
    "# encode method \n",
    "text = '''hello my name is david \n",
    "i am from nepal'''\n",
    "encode = tokenizer.encode(text)\n",
    "print(f'encode output : {encode}')\n",
    "print(f'encoded ids {encode.ids}')\n",
    "print(f'encoded token {encode.tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode\n",
    "decoded = tokenizer.decode(encode.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he ll o my name is da v id i am from ne p al'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: ['T', 'hi', 's', 'is', 'a', 's', 'i', 'mp', 'le', 'in', 'p', 'ut', 'to', 'b', 'e', 'to', 'k', 'en', 'iz', 'ed']\n",
      "Decoded string: T hi s is a s i mp le in p ut to b e to k en iz ed\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenizer a simple input\n",
    "from tokenizers.models import BPE\n",
    "tokenizer.model = BPE.from_file('vocab.json', 'merges.txt')\n",
    "encoding = tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding.tokens))\n",
    "\n",
    "decoded = tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded string: {}\".format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.31k/4.31k [00:00<00:00, 3.82MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.17k/2.17k [00:00<00:00, 5.31MB/s]\n",
      "Downloading readme: 100%|██████████| 7.59k/7.59k [00:00<00:00, 14.5MB/s]\n",
      "Downloading data: 100%|██████████| 84.1M/84.1M [00:55<00:00, 1.51MB/s]  \n",
      "Generating train split: 100%|██████████| 25000/25000 [00:05<00:00, 4802.07 examples/s] \n",
      "Generating test split: 100%|██████████| 25000/25000 [00:05<00:00, 4957.62 examples/s] \n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:05<00:00, 8575.99 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 677/677 [00:00<00:00, 2.50MB/s]\n",
      "Downloading data: 100%|██████████| 3.45M/3.45M [00:02<00:00, 1.25MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 429.96it/s]\n",
      "Generating train split: 100%|██████████| 20022/20022 [00:00<00:00, 945776.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset2 = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Create a function that takes a specific input and produces a specific output using any mathematical operators. Write corresponding code in Python.', 'input': '', 'output': 'def f(x):\\n    \"\"\"\\n    Takes a specific input and produces a specific output using any mathematical operators\\n    \"\"\"\\n    return x**2 + 3*x'}\n"
     ]
    }
   ],
   "source": [
    "for item in dataset2:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 395/395 [00:00<00:00, 1.27MB/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "Downloading data: 100%|██████████| 20.9M/20.9M [00:03<00:00, 6.00MB/s]\n",
      "Downloading data: 100%|██████████| 1.11M/1.11M [00:01<00:00, 851kB/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 500.27it/s]\n",
      "Generating train split: 9846 examples [00:00, 197081.83 examples/s]\n",
      "Generating test split: 518 examples [00:00, 124821.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# for chat dataset {'user','assistant'}\n",
    "\n",
    "dataset3 = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog'}\n"
     ]
    }
   ],
   "source": [
    "for item in dataset3:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
